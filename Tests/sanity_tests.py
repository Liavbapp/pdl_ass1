import unittest
import numpy as np
from Components import backward


class Tests(unittest.TestCase):

    def test_softmax(self):
        A_L = np.random.randn(5, 10)  # features=5,  m=10

        softmax_res = forward.softmax(A_L)
        self.assertTrue(softmax_res.shape == A_L.shape)
        np.testing.assert_allclose(softmax_res.sum(axis=0), np.ones(A_L.shape[1]))

    def test_safe_softmax(self):
        A_L = np.random.randn(5, 10)  # features=5,  m=10

        softmax_res = forward.safe_softmax(A_L)
        self.assertTrue(softmax_res.shape == A_L.shape)
        np.testing.assert_allclose(softmax_res.sum(axis=0), np.ones(A_L.shape[1]))

    def test_compute_softmax_gradient_vector_respect_to_weights(self):
        # X = np.random.rand(10, 4)  # features=10,  m=4
        #
        # W = np.random.rand(3, 10)  # cur=3, prev=10
        #
        # C = np.array([[1, 0, 1, 0],
        #               [0, 1, 0, 0],
        #               [0, 0, 0, 1]])  # l=3, m=4
        C = np.array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
                       1., 1., 1., 1.],
                      [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
                       0., 0., 0., 0.]])
        W = np.array([[-1.15533548e+00, 2.56725029e-01, -1.01203238e-03,
                       -9.32449353e-04, 1.08502882e-01, 1.98838879e-01,
                       2.54345482e-01],
                      [1.03890709e+00, -1.25727356e-01, -1.03086924e+00,
                       -3.24842783e-01, 1.24748056e-01, 1.08356174e+00,
                       -3.80595588e-02]])

        X = np.array([[0.00000000e+00, 0.00000000e+00, 7.19405072e-03, 0.00000000e+00,
                       7.04543055e-03, 9.41178080e-03, 5.55630979e-02, 2.12473008e-03,
                       1.53241685e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
                       1.15082128e-02, 0.00000000e+00, 6.41345985e-02, 1.82417302e-02,
                       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.91942234e-02],
                      [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.06788741e-03,
                       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
                       0.00000000e+00, 3.21854935e-03, 0.00000000e+00, 0.00000000e+00,
                       0.00000000e+00, 3.62266507e-02, 0.00000000e+00, 0.00000000e+00,
                       1.23913722e-02, 3.26258762e-05, 9.93604043e-03, 0.00000000e+00],
                      [0.00000000e+00, 0.00000000e+00, 3.58070916e-03, 0.00000000e+00,
                       4.72301534e-03, 4.07426752e-03, 0.00000000e+00, 0.00000000e+00,
                       9.44138778e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
                       1.04858907e-02, 4.81626061e-02, 0.00000000e+00, 0.00000000e+00,
                       3.67141377e-03, 0.00000000e+00, 9.61665731e-03, 1.53181921e-02],
                      [5.20585032e-03, 0.00000000e+00, 4.28706307e-03, 0.00000000e+00,
                       8.06683760e-03, 3.53019596e-03, 0.00000000e+00, 0.00000000e+00,
                       4.56682877e-03, 2.10019563e-03, 1.17695325e-02, 0.00000000e+00,
                       1.04316659e-02, 1.49834056e-03, 0.00000000e+00, 3.12281872e-02,
                       0.00000000e+00, 7.54442917e-03, 0.00000000e+00, 3.19714960e-03],
                      [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
                       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
                       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
                       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
                       6.34946465e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
                      [0.00000000e+00, 0.00000000e+00, 2.09209221e-03, 0.00000000e+00,
                       2.18929592e-04, 3.70557714e-03, 0.00000000e+00, 0.00000000e+00,
                       3.04193865e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
                       0.00000000e+00, 0.00000000e+00, 1.73679986e-02, 0.00000000e+00,
                       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.01170600e-02],
                      [8.59620318e-03, 2.33533590e-02, 2.11727657e-03, 2.22867745e-02,
                       0.00000000e+00, 3.91743742e-03, 4.35049174e-02, 1.84703018e-02,
                       2.10717552e-02, 3.63995915e-02, 1.45545254e-02, 3.62031311e-02,
                       2.52068948e-02, 6.57483275e-02, 6.96726245e-02, 8.66766470e-03,
                       4.44297167e-02, 2.96906398e-02, 2.27247501e-02, 3.27700951e-02]])

        actual_grads = backward.compute_softmax_gradient_vector_respect_to_weights(X, W, C)
        self.assertTrue(actual_grads.shape[0] == W.shape[0] and actual_grads.shape[1] == W.shape[1])

    def test_cross_entropy_softmax_lost(self):
        X = np.array([[0.52818428, 0.04495849, 0.55422462, 0.8550578],
                      [0.60724775, 0.3631421, 0.20652824, 0.95099002],
                      [0.2413483, 0.72092741, 0.28588687, 0.76531959],
                      [0.92140116, 0.26976411, 0.58529224, 0.21398077],
                      [0.60856064, 0.01444655, 0.68838315, 0.78568933]])  # n=5, m=4

        W = np.array([[9.42584723e-01, 3.87141862e-01, 5.65437122e-04, 7.10752235e-01, 5.08893974e-01],
                      [6.04889885e-01, 9.65193922e-01, 4.28946985e-01, 6.08191680e-01, 1.35084548e-01],
                      [2.54619842e-01, 1.78962612e-01, 8.01471760e-01, 6.80141444e-01, 2.35958096e-01]])  # n=3 , n-1=5

        C = np.array([[0, 0, 1, 0],
                      [1, 0, 0, 0],
                      [0, 1, 0, 1]])  # l=3, m=4
        expected_loss = 1.085366618369291
        actual_loss = forward.cross_entropy_softmax_lost(X, W, C, with_eta=False)
        np.testing.assert_allclose(expected_loss, actual_loss)

    def test_forward_pass_t1(self):
        X = np.random.rand(5, 3)  # n=5, m=3
        W = np.random.rand(2, 5)  # w1=2, w0=5
        W_dict = {'W1': W}
        A_L = forward.forward_pass(X, W_dict)
        self.assertTrue(A_L.shape[0] == 2, A_L.shape[1] == 3)
        np.testing.assert_allclose(np.sum(A_L, axis=0), np.ones(3))

    def test_forward_pass_t2(self):
        X = np.random.rand(5, 3)  # n=5, m=3
        print("X: ", X)
        W_1 = np.random.rand(2, 5)  # w1=2, w0=5
        print("W_1: ", W_1)
        W_2 = np.random.rand(4, 2)
        print("W_2: ", W_2)
        W_3 = np.random.rand(3, 4)
        print("W_3: ", W_3)
        W_dict = {'W1': W_1, 'W2': W_2, 'W3': W_3}
        A_L = forward.forward_pass(X, W_dict)
        print("A_L: ", A_L)
        self.assertTrue(A_L.shape[0] == 3, A_L.shape[1] == 3)
        np.testing.assert_allclose(np.sum(A_L, axis=0), np.ones(3))

    def test_forward_pass_t3(self):
        X = np.array([[0.23174121, 0.7610559, 0.49483376],
                      [0.36007492, 0.71299414, 0.41246607],
                      [0.90198136, 0.88417988, 0.47496868],
                      [0.54249147, 0.51142508, 0.34182941],
                      [0.79066538, 0.46137318, 0.94054406]])  # 5x3

        W_1 = np.array([[0.77145011, 0.68625424, 0.50337483, 0.86385026, 0.0135212],
                        [0.001316, 0.44814127, 0.41160443, 0.31282115, 0.52893298]])  # 2x5

        W_2 = np.array([[0.22006386, 0.91695532],
                        [0.93700986, 0.2066702],
                        [0.80240503, 0.16421343],
                        [0.81019019, 0.43283506]])  # 4x2

        W_3 = np.array([[0.59773755, 0.18292451, 0.54529299, 0.36539322],
                        [0.55707105, 0.02494888, 0.06766612, 0.39275127],
                        [0.28398953, 0.27095607, 0.27950803, 0.04073363]])  # 3x4

        W_dict = {'W1': W_1, 'W2': W_2, 'W3': W_3}
        A_L = forward.forward_pass(X, W_dict)

        expected_A_L = np.array([[0.57308885, 0.64268965, 0.54599659],
                                 [0.2431913, 0.199682, 0.25474035],
                                 [0.18371985, 0.15762834, 0.19926306]])

        self.assertTrue(A_L.shape[0] == 3, A_L.shape[1] == 3)
        np.testing.assert_allclose(A_L, expected_A_L)
        np.testing.assert_allclose(np.sum(A_L, axis=0), np.ones(3))
